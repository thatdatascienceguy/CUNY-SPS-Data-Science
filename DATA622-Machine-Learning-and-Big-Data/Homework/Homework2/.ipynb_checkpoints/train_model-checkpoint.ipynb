{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "Created on Sat Sep 21 13:25:33 2019\n",
    "\n",
    "@author: Jonathan Hernandez\n",
    "File that reads in the training titanic data and does the following in order:\n",
    "   1. Reads in the data\n",
    "   2. drops columns not needed in this case \n",
    "       'Survived', 'Name', 'Ticket', 'PassengerId', 'Cabin'\n",
    "       Besides Survived the dependent variable, the other 4 I felt were not \n",
    "       needed to determine or classify if a person survived the titanic crash\n",
    "       or not.\n",
    "   3. Maps gender values from M/F to 0's and 1's so we can train and fit a \n",
    "      random forest classifier that will classify if a passenger died (0) or\n",
    "      survived (0)\n",
    "   4. Replaces missing Embarked values with the most common Embarked status and\n",
    "      maps Embarked values from 0,1,2 and so on.\n",
    "   5. Replaces missing ages by the median of the ages in the training set.\n",
    "   6. Splits the training data into a training/test set to test and fit our\n",
    "      model.\n",
    "   7. Trains a random forest classifier model and then outputs a plot showing\n",
    "      which features are most important and makes predictions on the test set.\n",
    "   8. Writes the accuracy of the model to a text file and saves the \n",
    "      Random Forest model in a .pkl where it will be used to make predictions on \n",
    "      the test.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import our models \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\"\"\"Read in the dataset in question\"\"\"\n",
    "def read_data(training_dataset):\n",
    "    data = pd.read_csv(training_dataset)\n",
    "    return data\n",
    "\n",
    "\"\"\"Split the training set into a 70/30 rule\n",
    "That is, 70 percent of the data is used to train a Random Forest model and \n",
    "30% is used to test it out\n",
    "\"\"\"\n",
    "def titanic_train_data_split(X, y, split_percentage=0.7):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size = split_percentage, random_state=200)\n",
    "    return X_train, X_test, y_train, y_test # return the split data\n",
    "    \n",
    "\n",
    "\"\"\"Function to drop specified columns and returns\n",
    "the dataset excluding the dropped columns\"\"\"\n",
    "def drop_columns(dataset, list_of_cols_drop):\n",
    "    if list_of_cols_drop == []:\n",
    "        return dataset # nothing to do; simply return it\n",
    "    else:\n",
    "        data_X = dataset.drop(columns=list_of_cols_drop, axis=1)\n",
    "        # new dataset that omits if any columns to drop\n",
    "        return data_X\n",
    "\n",
    "\"\"\"Transform the gender values from Female/Male to 0 and 1\"\"\"\n",
    "def gender_mapping(dataset):\n",
    "    # convert male and female to 0 and 1 respectively for using a \n",
    "    # random forest classifier\n",
    "    gender_encoder = LabelEncoder()\n",
    "    dataset['Sex'] = gender_encoder.fit_transform(dataset['Sex'])\n",
    "    return dataset\n",
    "\n",
    "\"\"\"Replace missing Embarked values and map them to integers\"\"\"\n",
    "def embarked_status_mapping(dataset):\n",
    "    # looking at the Embarked column there are 2 missing values, let's first\n",
    "    # replace them with the most common Embarked status\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(\n",
    "            dataset['Embarked'].mode()[0])\n",
    "    \n",
    "    # convert each value of Embarked as numbers 0, 1, and 2 for using\n",
    "    # the random forest model\n",
    "    dataset['Embarked'] = pd.factorize(dataset['Embarked'])[0]\n",
    "    return dataset\n",
    "\n",
    "\"\"\"Replace missing ages with the median of ages\"\"\"    \n",
    "def impute_missing_age(dataset, ages):\n",
    "    ages.fillna(ages.median(), inplace = True)\n",
    "    #print(sum(ages.isnull()))\n",
    "    dataset['Age'] = ages\n",
    "    return dataset\n",
    "\n",
    "\"\"\"Plot Feature importance\"\"\"\n",
    "def plot_feature_imp(xlabel, ylabel, title, data, model):\n",
    "    feature_imp = pd.Series(model.feature_importances_,index=data.columns)\n",
    "    feature_imp = feature_imp.sort_values(ascending=False)\n",
    "    sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "if \"__main__\" == __name__:\n",
    "    train_titanic = pd.read_csv('train.csv') # read in the training data\n",
    "    train_survived = train_titanic['Survived'] # dependent variable\n",
    "    \n",
    "    # let's see how many missing values are in each column\n",
    "    train_titanic.isnull().sum()\n",
    "    \n",
    "    # Drop the columns belowa as these factors we will not use\n",
    "    # in our randomforest model\n",
    "    cols_drop = ['Survived', 'Name', 'Ticket', 'PassengerId', 'Cabin']\n",
    "    train_titanic = drop_columns(train_titanic, cols_drop)\n",
    "    \n",
    "    # Age and Embarked will have to be imputed    \n",
    "    # creatie mapped gender values from strings\n",
    "    train_titanic = gender_mapping(train_titanic)\n",
    "    \n",
    "    # replace Embarked strings with 0, 1, 2, ...\n",
    "    train_titanic = embarked_status_mapping(train_titanic)\n",
    "    # replace missing Age values with their median\n",
    "    train_titanic = impute_missing_age(train_titanic, train_titanic['Age'])\n",
    "    \n",
    "    # split the training and test data\n",
    "    train_X, test_X, train_y, test_y = titanic_train_data_split(train_titanic,\n",
    "                                                                train_survived)\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    # fit a Random forest classifier on the training portion of the training\n",
    "    # dataset\n",
    "    rfc.fit(train_X, train_y)    \n",
    "    \n",
    "    # feature plot\n",
    "    plot_feature_imp('Feature Importance Score',\n",
    "                     'Features',\n",
    "                     'Visualizing Important Features',\n",
    "                     train_X, rfc)\n",
    "    \n",
    "    # make predictions on the test_X dataset and see how accurate our model is\n",
    "    pred_y = rfc.predict(test_X)\n",
    "    accuracy_file = open(\"RandomForestClassifier.txt\", \"w\")\n",
    "    accuracy_file.write(\"Accuracy: %s\" % (metrics.accuracy_score(test_y, pred_y)))\n",
    "    accuracy_file.close()\n",
    "    \n",
    "    # save into a pickle (pkl file)\n",
    "    random_forest_model_filename = 'DATA622_titanic_random_forest_model.pkl'\n",
    "    random_forest_model_pkl = open(random_forest_model_filename, 'wb')\n",
    "    pickle.dump(rfc, random_forest_model_pkl)\n",
    "    random_forest_model_pkl.close()\n",
    "    # save into a pickle (pkl file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
