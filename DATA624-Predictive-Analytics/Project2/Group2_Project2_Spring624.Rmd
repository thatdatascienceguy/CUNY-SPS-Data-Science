---
title: "DATA 624 Project 2"
author: "Jonathan Hernandez"
output:
  html_document: default
  pdf_document: default
---

```{r libaries, echo=FALSE}
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(kableExtra)))
suppressWarnings(suppressMessages(library(elasticnet)))
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(e1071)))
suppressWarnings(suppressMessages(library(leaps)))
suppressWarnings(suppressMessages(library(glmnet)))
suppressWarnings(suppressMessages(library(vip)))

```

- Training and test data has been loaded and let's read it in
(The data have been imputed, removed correlated data etc)

Source: https://github.com/john-grando/data624_hw_group_2/tree/master/Project2

```{r readrdata, echo=FALSE}
load(file = "train.rda")
load(file = "test.rda")
```

- Preview the training data

```{r previewdata, echo=FALSE}
summary(train)
str(train)
dim(train)
```

Regression - Forward Selection

- A simple multiple linear regression via forward selection. The regsubsets() function
from the leaps package picks the best fit linear regression model and features using
1 feature, 2 features and so on. 

```{r regressionfowrardsel, echo=FALSE}
# go through every subset and possible combination of features for a linear model
lm_student_train <- regsubsets(PH ~ ., data = train,
                               nvmax = 32, method = "forward")

# let's loop through each one to see what is the best model from calculating
# the RMSE's from the subsets

# store RMSE's from each subset model
rmse <- rep(NA,29)
train.matrix <- model.matrix(PH ~ ., data=train)

for (i in 1:29){
  coef_i <- coef(lm_student_train, id = i) # get the coefficients of the model
  pred_i <- train.matrix[, names(coef_i)] %*% coef_i # training data model predictions
  rmse[i] <- sqrt(mean((pred_i - train$PH)^2))
}
# return the number of features that has the smalles RMSE from the best subset
# of linear models

# plot the RMSE for each number of features for each model

ggplot(data = data.frame(features=seq(1,29), rmse=rmse),
       aes(x=features,y=rmse)) +
  geom_point() +
  geom_line() +
  xlab("Number of Features") +
  ylab("") +
  ggtitle("RMSE for various features") +
  theme_bw() +
  theme_classic()

print(paste("Number of features that make the best model and rmse: ",
            which.min(rmse), sep = " "))

# list the rmse as well
print(paste("Smallest RMSE: ", round(rmse[which.min(rmse)], 4), sep = " "))

# list the coefficients of the model.
#coef(lm_student_train, which.min(rmse)) %>% kable() %>%
#  kable_styling(position = "center")

# recreate the model with the given coefficients
# this is so we can use the predict function to predict the PH levels in the test
# dataset
features <- names(coef(lm_student_train, which.min(rmse)))[6:(ncol(train)-2)] # take out the intercept

# formula to use to properly pass to predict() function
form <- as.formula(paste("PH", paste("Brand.Code +" ,
                                     paste(features, collapse = " + "))
                   ,sep = " ~ "))

lm_PH <- lm(form, data = train)
pred_PH_regsubsets <- round(predict(lm_PH, test), 2)
write.table(pred_PH_regsubsets,
            "predicted-PH-regsubsets.csv",
            col.names = c("PH"),
            row.names = F)

# Variable Importance
varImp_regsubsets <- varImp(lm_PH)
varImp_regsubsets
# Use vip function to do variable importance with lm objects
vip(lm_PH, num_features = 33L, method = "ice") +
  xlab("Importance") +
  ylab("")
```

- By using the regsubsets() function and looping through all feature number combinations,
we were able to find the right linear regression model that minimizes the RMSE between
the training data and using the know features and putting it into a lm object, we
were able to make predictions of the PH levels in the test dataset.

- Now, let's use elastic net with various parameters $\alpha$ and $\lambda$ using
the train() function

```{r elasticnet, echo=FALSE}
# use train() with 5-fold, repeated 5 times and find out the minimum RMSE
set.seed(123)
train_control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 5,
                              search = "random")

PH_enet_model <- train(PH ~.,
                       data = train,
                       method = "glmnet",
                       preProcess = c("center", "scale"),
                       tuneLength = 25,
                       trControl = train_control)

# coefficients of best model based on min(RMSE)
coefs <-coef(PH_enet_model$finalModel, PH_enet_model$bestTune$lambda)

# zero coefficients, removed
kable(coefs[,1][coefs[,1]==0])

  
# extract best RMSE value along with alpha, lambda
PH_enet_model$results[which.min(PH_enet_model$results[, "RMSE"]), ] %>%
  kable() %>% kable_styling(fixed_thead = T)

# see the RMSE between predicted PH in the training data and the actual PH
enet_pred_PH <- predict(PH_enet_model, newdata = train)
rmse_train_enet <- postResample(enet_pred_PH, train$PH)

# plot of actual PH vs predicted PH for the training dataset
ggplot(data = train, aes(x=train$PH, y=enet_pred_PH)) +
  geom_point() +
  geom_smooth(method = "glm") +
  xlab("Observed PH") +
  ylab("Predicted PH")

qplot(x=enet_pred_PH, y=residuals(PH_enet_model),
     xlab = "Predicted PH", ylab = "Residuals") +
  geom_hline(yintercept = 0)

# make predictions on the test dataset on PH
enet_pred_test_PH <- round(predict(PH_enet_model, newdata = test), 2)
write.csv(enet_pred_test_PH, "prediction-PH-enet.csv", col.names = "PH")

plot(varImp(PH_enet_model, scale = FALSE))
```